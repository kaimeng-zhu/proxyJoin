{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "df = pd.read_csv('qqpTrain.csv')\n",
    "\n",
    "data = df.to_dict(orient='records')\n",
    "\n",
    "data = data[:3500]\n",
    "newData = []\n",
    "bm25Query = []\n",
    "bm25Answer = []\n",
    "for idx1,d1 in enumerate(data):\n",
    "    bm25Query.append(d1[\"question1\"])\n",
    "    bm25Answer.append(d1[\"question2\"])\n",
    "    for idx2,d2 in enumerate(data):\n",
    "        isDuplicate = 0\n",
    "        if idx1 == idx2 and d1[\"is_duplicate\"] == 1:\n",
    "            isDuplicate = 1\n",
    "        newData.append({\"question1\":d1['question1'],\"question2\":d2['question2'],\"is_duplicate\":isDuplicate})\n",
    "data = newData\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' #build dataset\n",
    "import os\n",
    "import csv\n",
    "dataset_path = 'quora-dataset/'\n",
    "first5000 = []\n",
    "#counter = 0\n",
    "with open(os.path.join(dataset_path, 'classification', 'test_pairs.tsv'), 'r', encoding='utf8') as fIn:\n",
    "    reader = csv.DictReader(fIn, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "    for row in reader:\n",
    "        \n",
    "       \n",
    "        #if len(first5000) >= 5000:\n",
    "        #    break\n",
    "        first5000.append((row['question1'], row['question2'], row['is_duplicate']))\n",
    "        \n",
    "'''        \n",
    "'''\n",
    "pairs = []\n",
    "for Iidx in range(len(first5000)):\n",
    "    for Jidx in range(len(first5000)):\n",
    "        label = 0\n",
    "        if Iidx == Jidx and first5000[2] == 1:\n",
    "            label = 1\n",
    "        pairs.append((first5000[Iidx][0], first5000[Jidx][1], label))\n",
    "'''s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from scipy import spatial\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "w2vResult = []\n",
    "invalidList = set() #sync data between w2v and bm25\n",
    "try:\n",
    "    for idx,d in enumerate(first5000):\n",
    "        if type(d[0]) != str or type(d[1]) != str:\n",
    "            invalidList.add(idx)\n",
    "            continue\n",
    "\n",
    "        query = []\n",
    "        answer = []\n",
    "        for q in tokenizer.tokenize(d[0]):\n",
    "            if q in wv:\n",
    "                query.append(wv[q])\n",
    "        for a in tokenizer.tokenize(d[1]):\n",
    "            if a in wv:\n",
    "                answer.append(wv[a])\n",
    "        if not query or not answer:\n",
    "            invalidList.add(idx)\n",
    "            continue\n",
    "        query = np.max(np.array(query),0)\n",
    "        answer = np.max(np.array(answer),0)\n",
    "\n",
    "        doc_scores = 1 - spatial.distance.cosine(query, answer)\n",
    "        w2vResult.append((doc_scores, int(d[2])))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"qqpTestw2v.pkl\"\n",
    "\n",
    "# Serialize and save the list to a binary file using pickle\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(w2vResult, file)\n",
    "#'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Plus\n",
    "\n",
    "bm25Result = []\n",
    "for r in first5000:\n",
    "\n",
    "    tokenized_answer = [r[0].split(\" \")]\n",
    "    tokenized_question = r[1].split(\" \")\n",
    "    bm25 = BM25Plus(tokenized_answer)\n",
    "    doc_scores = bm25.get_scores(q)\n",
    "    bm25Result.append((doc_scores[0], int(r[2])))\n",
    "\n",
    "\n",
    "file_path = \"qqpTestbm25P.pkl\"\n",
    "\n",
    "# Serialize and save the list to a binary file using pickle\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(bm25Result, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''  # all data, not diag\n",
    "import gensim.downloader as api\n",
    "import pandas as pd\n",
    "\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "\n",
    "w2vResult = []\n",
    "invalidList = set() #sync data between w2v and bm25\n",
    "try:\n",
    "    for idx,d in enumerate(data):\n",
    "        if type(d['question1']) != str or type(d['question2']) != str:\n",
    "            invalidList.add(idx)\n",
    "            continue\n",
    "\n",
    "        query = []\n",
    "        answer = []\n",
    "        for q in tokenizer.tokenize(d['question1']):\n",
    "            if q in wv:\n",
    "                query.append(wv[q])\n",
    "        for a in tokenizer.tokenize(d['question2']):\n",
    "            if a in wv:\n",
    "                answer.append(wv[a])\n",
    "        if not query or not answer:\n",
    "            invalidList.add(idx)\n",
    "            continue\n",
    "        query = np.max(np.array(query),0)\n",
    "        answer = np.max(np.array(answer),0)\n",
    "\n",
    "        doc_scores = 1 - spatial.distance.cosine(query, answer)\n",
    "        w2vResult.append((doc_scores, d['is_duplicate']))\n",
    "        if d['is_duplicate'] == 1:\n",
    "            print(doc_scores)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"qqp5000w2v.pkl\"\n",
    "\n",
    "# Serialize and save the list to a binary file using pickle\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(w2vResult, file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' # fix old labels\n",
    "newbm25Result = []\n",
    "neww2vResult = []\n",
    "newBertResult = []\n",
    "for idx in range(len(labels)):\n",
    "    newbm25Result.append((bm25Result[idx][0],labels[idx]))\n",
    "    neww2vResult.append((w2vResult[idx][0], labels[idx]))\n",
    "    newBertResult.append((bertResult[idx][0], labels[idx]))\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"qqp5000bm25P.pkl\"\n",
    "\n",
    "# Serialize and save the list to a binary file using pickle\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(newbm25Result, file)\n",
    "\n",
    "\n",
    "# Define the file path\n",
    "file_path = \"qqp5000w2v.pkl\"\n",
    "\n",
    "# Serialize and save the list to a binary file using pickle\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(neww2vResult, file)\n",
    "\n",
    "\n",
    "    # Define the file path\n",
    "file_path = \"qqp500025xBert.pkl\"\n",
    "\n",
    "# Serialize and save the list to a binary file using pickle\n",
    "with open(file_path, \"wb\") as file:\n",
    "    pickle.dump(newBertResult, file)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#varification of inaccuracy\n",
    "iidx = 0\n",
    "jidx = 0\n",
    "counter = 0\n",
    "truePos = 0\n",
    "for idx, r in enumerate(fullBertResult):\n",
    "    if r[0] > 0.9 and r[1] == 0:\n",
    "        counter += 1\n",
    "        if counter == 2893:\n",
    "\n",
    "            iidx = idx//5000\n",
    "            jidx = idx%5000\n",
    "            print(r[0])\n",
    "            print(idx)\n",
    "            break\n",
    "    if r[0] > 0.9 and r[1] == 1:\n",
    "        truePos += 1\n",
    "print(counter)\n",
    "print(truePos)\n",
    "print(first5000[iidx][0])\n",
    "print(first5000[jidx][1])\n",
    "\n",
    "from sentence_transformers.cross_encoder import CrossEncoder\n",
    "import numpy as np\n",
    "#import os\n",
    "#import csv\n",
    "from sentence_transformers import InputExample\n",
    "#import pickle\n",
    "model = CrossEncoder('./output/training_quora-2023-09-28_16-05-03')\n",
    "print(model.predict([first5000[iidx][0],first5000[jidx][1]]))\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
